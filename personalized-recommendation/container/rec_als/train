#!/usr/bin/env python

from __future__ import print_function

import json
import os
import sys

import implicit
import pandas as pd
import scipy.sparse as sparse

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
#param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)



# The function to execute the training.
def train():
    print('Starting the training.')
    input_files = [os.path.join(training_path, file) for file in os.listdir(training_path)]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(training_path, channel_name))
    input_data = [pd.read_csv(file) for file in input_files]
    train_data = pd.concat(input_data)
    #raw_data = pd.read_csv()#df_train
    raw_data=train_data

    raw_data.columns = ['uuid', 'feature', 'score']

    #raw_data.tail()


    # Drop NaN columns
    data = raw_data.dropna()
    data = data.copy()

    # Create a numeric user_id and artist_id column
    data['uuid'] = data['uuid'].astype("category")
    data['feature'] = data['feature'].astype("category")
    data['uuid'] = data['uuid'].cat.codes
    data['feature'] = data['feature'].cat.codes
    sparse_item_user = sparse.csr_matrix((data['score'].astype(float), (data['feature'], data['uuid'])))
    sparse_user_item = sparse.csr_matrix((data['score'].astype(float), (data['uuid'], data['feature'])))

    # Initialize the als model and fit it using the sparse item-user matrix
    model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)

    # Calculate the confidence by multiplying it by our alpha value.
    alpha_val = 15
    data_conf = (sparse_item_user * alpha_val).astype('double')

    # Fit the model
    model.fit(data_conf)
    uuid_list = raw_data['uuid'].unique().tolist()
    get_index_uuid = []

    for i in range(len(uuid_list)):
        a = list(raw_data[raw_data['uuid'] == uuid_list[i]].index.values)
        get_index_uuid.append(a[0])

    uuid_val = []
    for i in range(len(uuid_list)):
        a = data['uuid'][get_index_uuid[i]]
        uuid_val.append(a)

    #get_index_uuid

    pred = []

    for i in range(0, len(uuid_list)):
        user_id = uuid_val[i]
        # Use the implicit recommender.
        recommended = model.recommend(user_id, sparse_user_item)

        features = []
        scores = []

        # Get artist names from ids
        for item in recommended:
            idx, score = item
            features.append(data.feature.loc[data.feature == idx].iloc[0])
            scores.append(score)
        f_index = []

        for i in range(len(features)):
            a = list(data[data['feature'] == features[i]].index.values)
            f_index.append(a[0])
        rec = []

        for i in range(len(f_index)):
            a = raw_data['feature'][f_index[i]]
            a = json.loads(a)
            # rec_1={}
            # rec_1["attr"]=a
            score = {"score": str(scores[i])}
            a.update(score)
            # rec_1["score"]=str(scores[i])
            # res=rec_1#json.dumps(rec_1)
            rec.append(a)
            # rec[a]=scores[i]
            # result=json.dumps(rec)
        # rec=json.dumps(rec)
        rec_list={}
        rec_list["uuid"]=uuid_list[i]
        rec_list["recommendation_list"]=rec
        final_result = json.dumps({"payload": rec_list})
        pred.append(final_result)

    output_path_json=output_path+"pred.json"
    with open(output_path_json, "w") as outfile:
        json.dump(pred, outfile)
    #json.dump(pred, open(r"final_recommendation/prediction/{}.json".format(client_id), "w"))



if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)